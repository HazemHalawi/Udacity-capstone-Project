{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6805fc316dd533ef78d1ed15cb200f44ac3ab15b"
   },
   "source": [
    "# Quora insincere question classifier\n",
    " Detect toxic content to improve online conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "d23d254455047aaf8c6482b888b0f40b1c6da521"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HazemH\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a7599b339f0c29d0686046443d9fff6683a3bf6a"
   },
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "5e0687883b476fad302b1fcfc641aca315e22934"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../Dataset/train.csv\")\n",
    "#X_train.head()\n",
    "#X_train.shape\n",
    "\n",
    "test_set = pd.read_csv(\"../Dataset/test.csv\")\n",
    "#test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e28b583a12870b06663c1f4322d546e8d478e6a6"
   },
   "source": [
    "Use only a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "988177bf9db0e155961d544806f588016acc8667"
   },
   "outputs": [],
   "source": [
    "X_train = X_train[:1000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00ed6b71fe9a64eb5a2decb23f7a55cc30ac563e"
   },
   "source": [
    "## 2. Preprocess question_text column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "708fd283c717ef29682b6501b4c2b376b6c0d284"
   },
   "source": [
    "Split the data into features and target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "e8e3c9a0283c1e22b53be97048a84bd9665e669e"
   },
   "outputs": [],
   "source": [
    "y_train = X_train['target']\n",
    "X_train = X_train[\"question_text\"]\n",
    "#X_train = X_train.drop('target', axis = 1)\n",
    "\n",
    "test_set = test_set[\"question_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6eada51b1478aa12fd11447a02a35de6161c41c"
   },
   "source": [
    "Add space and stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "d1295a1d0acf75d94402ea5aa5d37ccc2640ec34"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "X_train = X_train.apply(lambda x: str(x).translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation })))\n",
    "test_set = test_set.apply(lambda x: str(x).translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation })))\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "X_train = X_train.apply(lambda x: stemmer.stem(x))\n",
    "test_set = test_set.apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79b2cbd1e3d172a5842e01e9c03b4682d1b74f54"
   },
   "source": [
    "## 3. Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e189af04a8f7303090c0af7985807b354ab4543c"
   },
   "source": [
    "Define and fit tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f57e3d51345e6793a08261e2d5089d36a0c2ac04"
   },
   "outputs": [],
   "source": [
    "fullData = pd.concat([X_train,test_set],ignore_index=True)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(fullData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d860a923d767db9e864d772f88bc9f17c0c55934"
   },
   "source": [
    "Transform Label encode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b111526180f360b616c1a5bf388d0720f9d1411c"
   },
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "test_set = tokenizer.texts_to_sequences(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af6dd8fbbd68326d5bb5121f529bd78305b2bbc3"
   },
   "source": [
    "## 4. One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff153f6f24368a4671091ad8e3687d2e1b83d0c7"
   },
   "source": [
    "One-Hot Encode each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "113de61d520792e4d828ad127b2cbd4b61b6fcf4"
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "# One-hot encoding the output into vector mode, each of length num_words\n",
    "tokenizer = Tokenizer(num_words=500)\n",
    "\n",
    "X_train = tokenizer.sequences_to_matrix(X_train, mode='binary')\n",
    "test_set = tokenizer.sequences_to_matrix(test_set, mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4eef6fd4a325042351b37958f3fd1568bc7ced84"
   },
   "source": [
    "One hot encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "edadd32ac3c6e955100ec56bb955c1355b9a9cf1"
   },
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "658611ff204c2c5dfabd56f0541e57309d9d5f65"
   },
   "source": [
    "## 5. Split features into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4de39ff8b88d05f702e283134810833bea82b0a5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, X_test, y_train, y_test = train_test_split(X_train, \n",
    "                                                    y_train, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3bcce152d12c8f69b961bb32f83a5409ab6e96ad"
   },
   "source": [
    "Define the f1 score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c56d0be21146cd2c04556dcae0b42ae4db9eeb8"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "92348727f06ff47da24835fdf4de3532a01540d5"
   },
   "source": [
    "## 6. Define and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fc5c5cb228cefe627a51adbfe6988f53c18d0e04"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "# Build the model architecture\n",
    "QuoraClassifier = Sequential()\n",
    "\n",
    "# Adding layers\n",
    "# x_train.shape[1] = 500 = length of the one hot encoded vector.\n",
    "QuoraClassifier.add(Dense(128, input_dim= x_train.shape[1]))\n",
    "QuoraClassifier.add(Activation('tanh'))\n",
    "\n",
    "# Adding output layer\n",
    "# Dense will be equal to 2, since we one-hot-encoded the labels consisting of two values (0 and 1)\n",
    "QuoraClassifier.add(Dense(2))\n",
    "QuoraClassifier.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "# Compile the model using a loss function and an optimizer.\n",
    "QuoraClassifier.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics = [f1])\n",
    "\n",
    "\n",
    "QuoraClassifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0245f77fbfa69e6977d785c700ee581a79aa34a0"
   },
   "outputs": [],
   "source": [
    "history = QuoraClassifier.fit(x_train, y_train.values, nb_epoch=20, batch_size=100, verbose=1, validation_data=(X_test, y_test.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b717ee4361acea508e3c939f5bb2c10a19651cb3"
   },
   "source": [
    "## 7. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48ebbef1cfea041be8ffdd6522349245153b8a0a"
   },
   "outputs": [],
   "source": [
    "score = QuoraClassifier.evaluate(X_test, y_test.values, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "160b4ef96ebc69d4420c9ae690f769dd90523e0a"
   },
   "source": [
    "## 8. Predicting on the test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a867b27d8e1a2eee543d492474340d4ab40fc8c1"
   },
   "source": [
    "Transform it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f8a3a8229d67f782f56ddff753db0396f4f52816"
   },
   "outputs": [],
   "source": [
    "test_pred = QuoraClassifier.predict(test_set)\n",
    "test_pred = (test_pred > 0.35).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8942b84331e6b22dd6246333f59c964eb8520b88"
   },
   "source": [
    "Transform it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ffa291e66935f787ef3ef012f0bd1f610f2a1c8f"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=test_pred)\n",
    "#convert one hot encoded to original values\n",
    "s2 = df.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dc6dcf45c25b93e4302c340b7dae1a21639d1111"
   },
   "source": [
    "Save Predictions to Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64e343329c46b565f1581f6045527d42c06beb31"
   },
   "outputs": [],
   "source": [
    "#save to submission\n",
    "submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "submission['prediction'] = s2\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d02d9cb72e539dea54c09d0946b2f3e069f0e6d0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
